{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01978,
     "end_time": "2021-01-22T14:44:35.635385",
     "exception": false,
     "start_time": "2021-01-22T14:44:35.615605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is fully copied and translated to Korean from [@abhinand05's notebook](https://www.kaggle.com/abhinand05/vision-transformer-vit-tutorial-baseline).\n",
    "If there is an issue, let me know then I will revise it or delete this notebook.\n",
    "\n",
    "이 노트북은 [@abhinand05's 노트북](https://www.kaggle.com/abhinand05/vision-transformer-vit-tutorial-baseline)에서 전부 복사되고 한글로 번역하였습니다. 만약 문제가 있다면, 해당 부분을 수정하거나 이 노트북을 삭제하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-01-22T14:44:35.679323Z",
     "iopub.status.busy": "2021-01-22T14:44:35.678656Z",
     "iopub.status.idle": "2021-01-22T14:44:47.680870Z",
     "shell.execute_reply": "2021-01-22T14:44:47.681364Z"
    },
    "papermill": {
     "duration": 12.028138,
     "end_time": "2021-01-22T14:44:47.681559",
     "exception": false,
     "start_time": "2021-01-22T14:44:35.653421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nb_black\r\n",
      "  Downloading nb_black-1.0.7.tar.gz (4.8 kB)\r\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from nb_black) (7.13.0)\r\n",
      "Requirement already satisfied: black>='19.3' in /opt/conda/lib/python3.7/site-packages (from nb_black) (19.10b0)\r\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (4.4.2)\r\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (0.15.2)\r\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (3.0.5)\r\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (4.3.3)\r\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (0.7.5)\r\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (4.8.0)\r\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (2.6.1)\r\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->nb_black) (0.1.0)\r\n",
      "Requirement already satisfied: toml>=0.9.4 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (0.10.0)\r\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (1.4.3)\r\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (0.8.0)\r\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (2020.4.4)\r\n",
      "Requirement already satisfied: click>=6.5 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (7.1.1)\r\n",
      "Requirement already satisfied: typed-ast>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (1.4.1)\r\n",
      "Requirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from black>='19.3'->nb_black) (19.3.0)\r\n",
      "Requirement already satisfied: parso>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython->nb_black) (0.5.2)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->nb_black) (0.1.9)\r\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->nb_black) (0.2.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->nb_black) (1.14.0)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->nb_black) (0.6.0)\r\n",
      "Building wheels for collected packages: nb-black\r\n",
      "  Building wheel for nb-black (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for nb-black: filename=nb_black-1.0.7-py3-none-any.whl size=5280 sha256=fac17094fdeb944d221c255750a75be82e38f990398e1331dd0c3def2b44828e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/b2/88/51c66d23ea5fd0d40ed50997555e15d981d92671376a9a412a\r\n",
      "Successfully built nb-black\r\n",
      "Installing collected packages: nb-black\r\n",
      "Successfully installed nb-black-1.0.7\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"!cp -r ../input/vittutorialillustrations/* ./ \\n\\n!pip install nb_black\\n%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"!cp -r ../input/vittutorialillustrations/* ./ \\n\\n!pip install nb_black\\n%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!cp -r ../input/vittutorialillustrations/* ./ \n",
    "\n",
    "!pip install nb_black\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022269,
     "end_time": "2021-01-22T14:44:47.727027",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.704758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 서론\n",
    "\n",
    "이 노트북은 제목과 같이 두 파트로 나뉩니다.\n",
    "\n",
    "<a href=\"#Vision-Transformers:-A-gentle-introduction\">1. Vision Transformer란?</a> <br>\n",
    "<a href=\"#Vision-Transformer-Implementation-in-PyTorch\">2. 파이토치 구현</a>\n",
    "\n",
    "**Pytorch에서 ViT를 구현하기 전에 Vision Transformers의 기본 아이디어와 작동 방식을 간략히 설명하겠습니다.**\n",
    "\n",
    "따라서 코드에만 관심이 있다면 이 노트북의 두 번째 세션부터 읽으세요. [rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models) 라이브러리가 pretrained weights를 포함한 모델 구현을 모두 포함하고 있기때문에, 구현은 기존 방식과 크게 다르지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.025308,
     "end_time": "2021-01-22T14:44:47.776013",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.750705",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size=4 color='blue'>이 노트북이 도움이 된다면, 추천부탁드립니다.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022287,
     "end_time": "2021-01-22T14:44:47.820915",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.798628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vision Transformers란?\n",
    "\n",
    "Vision Transformers(ViT)는 2020년 10월 구글 브레인팀이 발표한 [AN IMAGE IS WORTH 16X16 WORDS:\n",
    "TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE 논문](https://arxiv.org/pdf/2010.11929.pdf)에서 소개되었습니다.\n",
    "\n",
    "ViT가 어떻게 동작하는지 이해하기 위해서 Transformers의 작동방식과 해결가능한 문제에 대해 사전지식이 필요합니다. Vit의 디테일한 설명 이전에 어떻게 transformers가 동작하는지 먼저 설명하겠습니다.\n",
    "\n",
    "![ViT-Illustration](vision-transformer.png)\n",
    "\n",
    "만약 NLP가 처음이고 transformer 모델에 대해 배우고 실제로 어떻게 작동하는지 직관적인 설명을 알고 싶다면 이 [블로그 포스트](https://jalammar.github.io/)를 보는 걸 추천드립니다. (위 이미지 또한 이 블로그 포스트 글에서 인용하였습니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022361,
     "end_time": "2021-01-22T14:44:47.866045",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.843684",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Transformers: 간략한 설명\n",
    "\n",
    "> **만약 Transformers에 대해 이해하고 있다면, 이 파트를 넘어가셔도 좋습니다.**\n",
    "\n",
    "Transformer는 자연어 처리문제를 혁신하였습니다. 처음 소개가 되었을 때, 여러 NLP 점수기록을 갱신하고 최고 성적을 달성하였습니다. 이제는 현대 NLP의 실질적인 표준이 되었고 LSTM 및 GRU와 같은 이전 세대 모델에 비해 큰 성능향상을 이루었습니다.\n",
    "\n",
    "NLP를 바꾼 가장 중요한 논문은 [\"Attention is all you need\"](https://arxiv.org/pdf/1706.03762.pdf) 입니다. Transformer 구조가 이 논문에서 소개되었기 때문이죠.\n",
    "\n",
    "## **Motivations:**\n",
    "\n",
    "squence와 NLP 문제를 위한 이전의 모델들은 발전된 RNN의 형태를 지닙니다. **이 모델들의 문제점은 long-term dependencies를 이해할 수 없습니다.**\n",
    "\n",
    "RNN의 변형인 LSTM과 GRU은 그러한 depencies를 이해할 순 있지만 제한적입니다.\n",
    "\n",
    "그러므로 Transformer의 motivation은 이러한 재발을 없애고 거의 모든 dependencies를 포착하는 것이었습니다. 더욱 정확히 말하면, transformer의 시야는 입력 전체입니다. 이러한 *self-attention(multi-headed)* 이라고 불리는 attention mechanism은 성능 향상의 주된 이유였습니다. 다른 이점 중 하나는 데이터를 병렬적으로 처리할 수 있다는 것입니다.\n",
    "\n",
    "\n",
    "### Transformer 구조\n",
    "**참고: 이 후 설명에 들어가는 각 단계에 대한 번호가 이 그림에 표시되어 있습니다.**\n",
    "\n",
    "![TranformerArchitecture](transformer-arch.png)\n",
    "\n",
    "- 트랜스포머에는 위의 다이어그램에서 왼쪽에 있는 디코더와 오른쪽에 있는 인코더라는 두 가지 부분이 있습니다. \n",
    "- 지금 우리가 기계번역을 하고 있다고 상상해보세요.\n",
    "- 인코더는 입력 데이터(문장)를 가져와서 입력의 중간 표현을 생성합니다. \n",
    "- 디코더는 이 중간 표현을 단계별로 디코딩하고 출력을 생성합니다. 그러나 이 둘의 차이점은 작동방식입니다.\n",
    "- ViT에는 인코더 섹션을 이해하는 것으로 충분합니다.\n",
    "\n",
    "> **참고: 이 설명은 모델 구조에 대한 직관적인 설명입니다. 좀 더 자세한 내용은 각각의 논문을 확인해주세요.**\n",
    "\n",
    "### Tranformers: 단계별 설명\n",
    "**(1)** 입력 데이터는 먼저 벡터에 임베딩됩니다. 임베딩 레이어는 각 단어에 대해 vector representation을 나타내는 데 도움이 됩니다.\n",
    "\n",
    "**(2)** 다음 단계에서 위치 인코딩이 입력 임베딩에 들어갑니다. 이는 Transformer가 입력으로 전달되는 시퀀스 순서(예: 문장)에 대해 전혀 모르기 때문입니다.\n",
    "\n",
    "**(3)** 이제 multi-headed attention은 값들이 조금씩 달라지는 부분입니다.\n",
    "\n",
    "**Multi-headed-attention 구조:**\n",
    "![multi-headed-attn](multi-headed-attention.png)\n",
    "\n",
    "**(4)** Multi-Headed Attention 은 세 가지 학습 가능한 벡터로 구성됩니다. Query, Key 및 Value 벡터입니다. 이는 검색(Query)과 검색 엔진이 Query를 Key와 비교하고 Value로 응답하는 정보 검색에서 비롯된다고 합니다.\n",
    "\n",
    "**(5)** Q와 K 표현은 한 단어가 다른 모든 단어에 얼마나 많이 참여해야 하는지를 나타내는 점수 행렬을 생성하기 위해 내적을 합니다. 점수가 높으면 더 많은 관심을 받고 그 반대도 됩니다.\n",
    "\n",
    "**(6)** 그런 다음 Q 및 K 벡터의 차수에 따라 점수 행렬이 축소됩니다. 이는 곱셈이 폭발적 효과를 가질 수 있기 때문에 보다 안정적인 gradients를 보장하기 위한 것입니다.\n",
    "\n",
    "(디코더를 설명할 때, Mask 부분에 대해 설명하겠습니다.)\n",
    "\n",
    "**(7)** 다음으로 점수 행렬이 Softmax되어 Attention 점수를 확률로 변환합니다. 더 높은 점수는 단어의 관심이 상승되고 낮은 점수는 하락합니다. 이렇게 하면 모델이 어떤 단어를 많이, 혹은 적게 사용할지 결정할 수 있습니다.\n",
    "\n",
    "**(8)** 그런 다음 확률이 있는 결과 행렬에 Value 벡터를 곱합니다. 이렇게 하면 Model이 학습한 확률 점수가 더 중요해집니다. 점수가 낮은 단어들은 효과적으로 빠져 나와 영향력이 낮아질 것입니다.\n",
    "\n",
    "**(9)** 그런 다음 QK 및 V 벡터의 연결된 출력을 선형 레이어에 공급하여 더 자세히 처리합니다.\n",
    "\n",
    "**(10)** Self-Attention이 시퀀스의 각 단어에 대해 이루어집니다. Self-Attention은 다른 Self-Attention에 의존하지 않기 때문에 Self-Attention 모듈의 복사본을 사용하여 모든 것을 동시에 처리할 수 있습니다. **(multi-headed)**.\n",
    "\n",
    "**(11)** 그런 다음 출력 값 벡터가 연결되고 입력 계층에서 유입되는 residual connection에 추가되며 그 결과 representation은 정규화를 위해 *LayerNorm*로 전달됩니다. (residual connection은 gradient가 네트워크를 통해 흐를 수 있도록 도와주며 LayernNorm은 교육 시간을 약간 줄이고 네트워크를 안정시키는 데 도움이 됩니다.)\n",
    "\n",
    "**(12)** 또한 출력 값을 포인트별 feed forward 네트워크로 전달하여 더 풍부한 representation을 얻습니다.\n",
    "\n",
    "**(13)** 출력이 다시 LayerNorm을 통과하고 이전 레이어에서 residual이 추가됩니다.\n",
    "\n",
    "\n",
    "**참고: 인코더 섹션을 마치면 Vision Transformer를 완전히 이해할 수 있습니다. 인코딩 계층과 매우 유사하기 때문에 디코더 부분을 이해하도록 하겠습니다.**\n",
    "\n",
    "**(14)** 디코더 부분에서는 인코더 결과값이 masked-multi headed attention를 거쳐 그 다음 attention layer로 들어갑니다. 이전 time step 혹은 단어의 입력의 인코더의 결과가 있는 경우에는 같이 들어갑니다.\n",
    "\n",
    "**(15)** 모델이 디코딩 중에 나중에 나올 단어를 확인할 수 없으므로 누출이 발생하지 않도록 Masked multi headed attention가 필요합니다. 이 작업은 점수 행렬의 나중에 나오는 단어 항목을 마스킹하여 수행됩니다. 현재 단어와 이전 단어들을 1로 추가하고 미래 단어 점수를 -inf로 추가합니다. 이렇게 하면 Softmax로 확률을 얻으면 나머지는 그대로 유지할 때 미래 단어는 0으로 감소됩니다.\n",
    "\n",
    "**(16)** Gradient의 흐름을 개선하기 위한 Residual Connection도 있습니다. 마지막으로 출력이 선형 레이어로 전송되고 Softmax되어 확률로 출력을 얻습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022238,
     "end_time": "2021-01-22T14:44:47.911096",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.888858",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vision Transformers는 어떻게 작동하나요?\n",
    "\n",
    "이제 우리는 Transformers의 구조를 자세히 다루었으므로, Vision Transformers에 대한 문제를 해결할 준비가 되었습니다.\n",
    "\n",
    "이미지에 트랜스포머를 적용하는 것은 아래와 같은 이유로 어려운 문제였습니다.\n",
    "- 단어/문장/문단과는 달리, 이미지는 기본적으로 픽셀 형태로 훨씬 더 많은 정보를 포함합니다.\n",
    "- 고사양 하드웨어가 있더라도 이미지의 다른 모든 픽셀을 인식하기는 매우 어려울 것입니다.\n",
    "- 이에 대한 대안은 localized attention을 사용하는 것이었습니다. \n",
    "- 사실 CNN은 컨볼루션을 통해 매우 유사한 기능을 하고 인지 범위는 모델의 레이어에 깊이 들어갈수록 더 커지지만 Transformer는 'Transformer'의 특성상 CNN보다 항상 계산적으로 더 비쌀 것입니다. 물론, 우리는 CNN이 현재의 컴퓨터 비전 발전에 얼마나 많은 기여를 했는지 알고 있습니다.\n",
    "\n",
    "구글 연구원들은 그들의 논문에서 컴퓨터 비전의 다음 큰 단계가 될 수 있는 것으로 다른 구조를 제안했습니다. 그들은 CNN에 대한 의존이 더 이상 필요하지 않을 수도 있다는 것을 보여줍니다. 이제 Vision Transformers에 대해 자세히 살펴보겠습니다.\n",
    "\n",
    "### Vision Transformer 구조\n",
    "\n",
    "![vit-architecture](vit-arch.png)\n",
    "\n",
    "**(1)** 이들은 Transformer의 인코더 부분만 사용하지만, 다른 점은 이미지를 네트워크에 입력하는 방식에 있습니다.\n",
    "\n",
    "**(2)** 이미지를 고정 크기의 패치들로 나누었습니다. 따라서 이러한 패치 중 하나는 16x16 또는 32x32가 될 수 있습니다. 패치가 많을수록 패치 자체의 크기가 작아질수록 이러한 모델을 더 쉽게 교육할 수 있습니다. 그래서 \"An Image is worth 16x16 words\"라는 제목은 이러한 점을 나타냅니다.\n",
    "\n",
    "**(3)** 패치는 flattened되고 추가 처리를 위해 모델로 입력됩니다.\n",
    "\n",
    "**(4)** 여기에서는 NN과 달리 모델이 시퀀스의 샘플 위치에 대해 전혀 알지 못합니다. 여기서 각 샘플은 입력 이미지의 패치입니다. 따라서 이미지는 **positional embedding vector** 와 함께 인코더로 공급됩니다. 여기서 주목해야 할 한 가지는 위치 임베딩도 학습할 수 있다는 것입니다. 따라서 하드 코딩된 벡터를 해당 위치에 실제로 공급하지 않습니다.\n",
    "\n",
    "**(5)** 시작 부분에는 BERT와 같은 특수 토큰도 있습니다.\n",
    "\n",
    "**(6)** 따라서 각 이미지 패치는 먼저 큰 벡터로 flattened되고 학습 가능한 embedding matrix와 함께 곱해져 embedded patch를 생성합니다. 그리고 이 embedded patch는 positional embedding vector와 결합되어 Transformer에 공급됩니다.\n",
    "\n",
    "> **참고: 여기서부터는 모든 것이 기존 Transformer와 동일합니다**\n",
    "\n",
    "**(7)** 유일한 차이점은 디코더 대신 인코더의 출력을 feed-forward 신경망으로 직접 전달하여 분류 출력을 얻는 것입니다.\n",
    "\n",
    "### 유의할 사항:\n",
    "- 본 논문은 컨볼루션에 대해 거의 완전히 무시합니다. \n",
    "- 그러나 이미지 패치의 컨볼루션 embedding이 사용되는 ViT의 두 가지 변형을 사용하고 있습니다. 하지만 성능에 큰 영향을 미치지 않는 것 같습니다.  \n",
    "- 이 문서를 작성할 당시 Vision Transformers는 ImageNet의 이미지 분류 벤치마크에서 1위를 차지했습니다.\n",
    "\n",
    "<img src=\"benchmarks-chart.png\" width=\"700\">\n",
    "<!-- ![BenchmarksChart](benchmarks-charpng) -->\n",
    "\n",
    "- 본 논문에서는 훨씬 더 흥미로운 점이 많지만, CNN을 통해 Transformer의 힘을 보여 줄 수 있는 한 가지는 아래 이미지에 나와 있습니다. 아래 이미지는 레이어에 대한 attention distance를 보여줍니다.\n",
    "\n",
    "<img src=\"attn-distance.png\" width=\"300\" height=\"300\">\n",
    "<br>\n",
    "\n",
    "- 위의 그래프는 Transformer가 시작부터 멀리 떨어져 있는 영역에 대해 이미 Attention을 나타낼 수 있음을 시사하며, 이는 처음에 제한된 수신 필드를 가진 CNN에 비해 Transformer가 가진 상당한 장점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024107,
     "end_time": "2021-01-22T14:44:47.957784",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.933677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <font size=4 color='blue'>이 노트북이 도움이 된다면, 추천부탁드립니다.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022434,
     "end_time": "2021-01-22T14:44:48.002909",
     "exception": false,
     "start_time": "2021-01-22T14:44:47.980475",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vision Transformer 파이토치 구현\n",
    "이제 Vision Transformer에 대해 이해하였으니 이제 [이 대회](https://www.kaggle.com/c/cassava-leaf-disease-classification)를 위한 베이스라인을 구축해 보겠습니다.\n",
    "\n",
    "먼저 torch-images-models(timm)과 TPU를 사용하기 위해 torch-xla를 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-01-22T14:44:48.060514Z",
     "iopub.status.busy": "2021-01-22T14:44:48.059841Z",
     "iopub.status.idle": "2021-01-22T14:46:28.338845Z",
     "shell.execute_reply": "2021-01-22T14:46:28.338171Z"
    },
    "papermill": {
     "duration": 100.313384,
     "end_time": "2021-01-22T14:46:28.338972",
     "exception": false,
     "start_time": "2021-01-22T14:44:48.025588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\r\n",
      "100  5116  100  5116    0     0  36542      0 --:--:-- --:--:-- --:--:-- 36805\r\n",
      "Updating... This may take around 2 minutes.\r\n",
      "Updating TPU runtime to pytorch-1.7 ...\r\n",
      "Found existing installation: torch 1.5.0\r\n",
      "Uninstalling torch-1.5.0:\r\n",
      "  Successfully uninstalled torch-1.5.0\r\n",
      "Found existing installation: torchvision 0.6.0a0+35d732a\r\n",
      "Uninstalling torchvision-0.6.0a0+35d732a:\r\n",
      "Done updating TPU runtime\r\n",
      "  Successfully uninstalled torchvision-0.6.0a0+35d732a\r\n",
      "Copying gs://tpu-pytorch/wheels/torch-1.7-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/114.2 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/127.4 MiB.                                    \r\n",
      "Copying gs://tpu-pytorch/wheels/torchvision-1.7-cp37-cp37m-linux_x86_64.whl...\r\n",
      "\r\n",
      "Operation completed over 1 objects/3.1 MiB.                                      \r\n",
      "Processing ./torch-1.7-cp37-cp37m-linux_x86_64.whl\r\n",
      "Collecting dataclasses\r\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (3.7.4.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (1.18.5)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==1.7) (0.18.2)\r\n",
      "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: dataclasses, torch\r\n",
      "Successfully installed dataclasses-0.6 torch-1.7.0a0\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torch_xla-1.7-cp37-cp37m-linux_x86_64.whl\r\n",
      "Installing collected packages: torch-xla\r\n",
      "Successfully installed torch-xla-1.7\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "Processing ./torchvision-1.7-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==1.7) (1.7.0a0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==1.7) (1.18.5)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==1.7) (7.2.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==1.7) (0.18.2)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==1.7) (0.6)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==1.7) (3.7.4.1)\r\n",
      "Installing collected packages: torchvision\r\n",
      "Successfully installed torchvision-0.9.0a0+75e4a7d\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "The following NEW packages will be installed:\r\n",
      "  libomp5\r\n",
      "0 upgraded, 1 newly installed, 0 to remove and 59 not upgraded.\r\n",
      "Need to get 234 kB of archives.\r\n",
      "After this operation, 774 kB of additional disk space will be used.\r\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\r\n",
      "Fetched 234 kB in 1s (307 kB/s)\r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "Selecting previously unselected package libomp5:amd64.\r\n",
      "(Reading database ... 107745 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\r\n",
      "Unpacking libomp5:amd64 (5.0.1-1) ...\r\n",
      "Setting up libomp5:amd64 (5.0.1-1) ...\r\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\r\n",
      "Collecting timm\r\n",
      "  Downloading timm-0.3.4-py3-none-any.whl (244 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 244 kB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.9.0a0+75e4a7d)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm) (1.7.0a0)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (7.2.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.18.5)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (3.7.4.1)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.6)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm) (0.18.2)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.3.4\r\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\\n!python pytorch-xla-env-setup.py --version 1.7\\n!pip install timm\";\n",
       "                var nbb_formatted_code = \"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\\n!python pytorch-xla-env-setup.py --version 1.7\\n!pip install timm\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --version 1.7\n",
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:28.446679Z",
     "iopub.status.busy": "2021-01-22T14:46:28.445949Z",
     "iopub.status.idle": "2021-01-22T14:46:30.501750Z",
     "shell.execute_reply": "2021-01-22T14:46:30.501215Z"
    },
    "papermill": {
     "duration": 2.114516,
     "end_time": "2021-01-22T14:46:30.501868",
     "exception": false,
     "start_time": "2021-01-22T14:46:28.387352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\nplt.style.use(\\\"ggplot\\\")\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision.transforms as transforms\\n\\nimport torch_xla\\nimport torch_xla.core.xla_model as xm\\nimport torch_xla.distributed.xla_multiprocessing as xmp\\nimport torch_xla.distributed.parallel_loader as pl\\n\\nimport timm\\n\\nimport gc\\nimport os\\nimport time\\nimport random\\nfrom datetime import datetime\\n\\nfrom PIL import Image\\nfrom tqdm.notebook import tqdm\\nfrom sklearn import model_selection, metrics\";\n",
       "                var nbb_formatted_code = \"import numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\n\\nplt.style.use(\\\"ggplot\\\")\\n\\nimport torch\\nimport torch.nn as nn\\nimport torchvision.transforms as transforms\\n\\nimport torch_xla\\nimport torch_xla.core.xla_model as xm\\nimport torch_xla.distributed.xla_multiprocessing as xmp\\nimport torch_xla.distributed.parallel_loader as pl\\n\\nimport timm\\n\\nimport gc\\nimport os\\nimport time\\nimport random\\nfrom datetime import datetime\\n\\nfrom PIL import Image\\nfrom tqdm.notebook import tqdm\\nfrom sklearn import model_selection, metrics\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "import timm\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import model_selection, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:30.608505Z",
     "iopub.status.busy": "2021-01-22T14:46:30.606946Z",
     "iopub.status.idle": "2021-01-22T14:46:30.615246Z",
     "shell.execute_reply": "2021-01-22T14:46:30.614537Z"
    },
    "papermill": {
     "duration": 0.063827,
     "end_time": "2021-01-22T14:46:30.615365",
     "exception": false,
     "start_time": "2021-01-22T14:46:30.551538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# TPU \\ubcd1\\ub82c\\ud654\\ub97c \\uc704\\ud55c \\uc124\\uc815\\nos.environ[\\\"XLA_USE_BF16\\\"] = \\\"1\\\"\\nos.environ[\\\"XLA_TENSOR_ALLOCATOR_MAXSIZE\\\"] = \\\"100000000\\\"\";\n",
       "                var nbb_formatted_code = \"# TPU \\ubcd1\\ub82c\\ud654\\ub97c \\uc704\\ud55c \\uc124\\uc815\\nos.environ[\\\"XLA_USE_BF16\\\"] = \\\"1\\\"\\nos.environ[\\\"XLA_TENSOR_ALLOCATOR_MAXSIZE\\\"] = \\\"100000000\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TPU 병렬화를 위한 설정\n",
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:30.720904Z",
     "iopub.status.busy": "2021-01-22T14:46:30.719892Z",
     "iopub.status.idle": "2021-01-22T14:46:30.739941Z",
     "shell.execute_reply": "2021-01-22T14:46:30.739118Z"
    },
    "papermill": {
     "duration": 0.073975,
     "end_time": "2021-01-22T14:46:30.740084",
     "exception": false,
     "start_time": "2021-01-22T14:46:30.666109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def seed_everything(seed):\\n    \\\"\\\"\\\"\\n    Seeds basic parameters for reproductibility of results\\n    \\n    Arguments:\\n        seed {int} -- Number of the seed\\n    \\\"\\\"\\\"\\n    random.seed(seed)\\n    os.environ[\\\"PYTHONHASHSEED\\\"] = str(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\n    torch.backends.cudnn.benchmark = False\\n\\n\\nseed_everything(1001)\";\n",
       "                var nbb_formatted_code = \"def seed_everything(seed):\\n    \\\"\\\"\\\"\\n    Seeds basic parameters for reproductibility of results\\n    \\n    Arguments:\\n        seed {int} -- Number of the seed\\n    \\\"\\\"\\\"\\n    random.seed(seed)\\n    os.environ[\\\"PYTHONHASHSEED\\\"] = str(seed)\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed(seed)\\n    torch.backends.cudnn.deterministic = True\\n    torch.backends.cudnn.benchmark = False\\n\\n\\nseed_everything(1001)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results\n",
    "    \n",
    "    Arguments:\n",
    "        seed {int} -- Number of the seed\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed_everything(1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:30.852018Z",
     "iopub.status.busy": "2021-01-22T14:46:30.851299Z",
     "iopub.status.idle": "2021-01-22T14:46:30.861895Z",
     "shell.execute_reply": "2021-01-22T14:46:30.862380Z"
    },
    "papermill": {
     "duration": 0.071074,
     "end_time": "2021-01-22T14:46:30.862541",
     "exception": false,
     "start_time": "2021-01-22T14:46:30.791467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# \\uc77c\\ubc18\\uc801\\uc778 \\uc804\\uc5ed \\ubcc0\\uc218\\ub4e4\\nDATA_PATH = \\\"../input/cassava-leaf-disease-classification\\\"\\nTRAIN_PATH = \\\"../input/cassava-leaf-disease-classification/train_images/\\\"\\nTEST_PATH = \\\"../input/cassava-leaf-disease-classification/test_images/\\\"\\nMODEL_PATH = (\\n    \\\"../input/vit-base-models-pretrained-pytorch/jx_vit_base_p16_224-80ecf9dd.pth\\\"\\n)\\n\\n# \\ubaa8\\ub378 \\uc804\\uc5ed \\ubcc0\\uc218\\nIMG_SIZE = 224\\nBATCH_SIZE = 16\\nLR = 2e-05\\nGAMMA = 0.7\\nN_EPOCHS = 10\";\n",
       "                var nbb_formatted_code = \"# \\uc77c\\ubc18\\uc801\\uc778 \\uc804\\uc5ed \\ubcc0\\uc218\\ub4e4\\nDATA_PATH = \\\"../input/cassava-leaf-disease-classification\\\"\\nTRAIN_PATH = \\\"../input/cassava-leaf-disease-classification/train_images/\\\"\\nTEST_PATH = \\\"../input/cassava-leaf-disease-classification/test_images/\\\"\\nMODEL_PATH = (\\n    \\\"../input/vit-base-models-pretrained-pytorch/jx_vit_base_p16_224-80ecf9dd.pth\\\"\\n)\\n\\n# \\ubaa8\\ub378 \\uc804\\uc5ed \\ubcc0\\uc218\\nIMG_SIZE = 224\\nBATCH_SIZE = 16\\nLR = 2e-05\\nGAMMA = 0.7\\nN_EPOCHS = 10\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 일반적인 전역 변수들\n",
    "DATA_PATH = \"../input/cassava-leaf-disease-classification\"\n",
    "TRAIN_PATH = \"../input/cassava-leaf-disease-classification/train_images/\"\n",
    "TEST_PATH = \"../input/cassava-leaf-disease-classification/test_images/\"\n",
    "MODEL_PATH = (\n",
    "    \"../input/vit-base-models-pretrained-pytorch/jx_vit_base_p16_224-80ecf9dd.pth\"\n",
    ")\n",
    "\n",
    "# 모델 전역 변수\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-05\n",
    "GAMMA = 0.7\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:30.975842Z",
     "iopub.status.busy": "2021-01-22T14:46:30.975077Z",
     "iopub.status.idle": "2021-01-22T14:46:31.024283Z",
     "shell.execute_reply": "2021-01-22T14:46:31.023729Z"
    },
    "papermill": {
     "duration": 0.108224,
     "end_time": "2021-01-22T14:46:31.024394",
     "exception": false,
     "start_time": "2021-01-22T14:46:30.916170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000015157.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000201771.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100042118.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000723321.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000812911.jpg</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_id  label\n",
       "0  1000015157.jpg      0\n",
       "1  1000201771.jpg      3\n",
       "2   100042118.jpg      1\n",
       "3  1000723321.jpg      1\n",
       "4  1000812911.jpg      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"df = pd.read_csv(os.path.join(DATA_PATH, \\\"train.csv\\\"))\\ndf.head()\";\n",
       "                var nbb_formatted_code = \"df = pd.read_csv(os.path.join(DATA_PATH, \\\"train.csv\\\"))\\ndf.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"train.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:31.140039Z",
     "iopub.status.busy": "2021-01-22T14:46:31.138296Z",
     "iopub.status.idle": "2021-01-22T14:46:31.155826Z",
     "shell.execute_reply": "2021-01-22T14:46:31.154989Z"
    },
    "papermill": {
     "duration": 0.078353,
     "end_time": "2021-01-22T14:46:31.155950",
     "exception": false,
     "start_time": "2021-01-22T14:46:31.077597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21397 entries, 0 to 21396\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   image_id  21397 non-null  object\n",
      " 1   label     21397 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 334.5+ KB\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"df.info()\";\n",
       "                var nbb_formatted_code = \"df.info()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:31.273605Z",
     "iopub.status.busy": "2021-01-22T14:46:31.272569Z",
     "iopub.status.idle": "2021-01-22T14:46:31.465869Z",
     "shell.execute_reply": "2021-01-22T14:46:31.465315Z"
    },
    "papermill": {
     "duration": 0.255871,
     "end_time": "2021-01-22T14:46:31.466012",
     "exception": false,
     "start_time": "2021-01-22T14:46:31.210141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa364806550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"df.label.value_counts().plot(kind=\\\"bar\\\")\";\n",
       "                var nbb_formatted_code = \"df.label.value_counts().plot(kind=\\\"bar\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.label.value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:31.587644Z",
     "iopub.status.busy": "2021-01-22T14:46:31.586805Z",
     "iopub.status.idle": "2021-01-22T14:46:31.617556Z",
     "shell.execute_reply": "2021-01-22T14:46:31.616838Z"
    },
    "papermill": {
     "duration": 0.09314,
     "end_time": "2021-01-22T14:46:31.617701",
     "exception": false,
     "start_time": "2021-01-22T14:46:31.524561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"train_df, valid_df = model_selection.train_test_split(\\n    df, test_size=0.1, random_state=42, stratify=df.label.values\\n)\";\n",
       "                var nbb_formatted_code = \"train_df, valid_df = model_selection.train_test_split(\\n    df, test_size=0.1, random_state=42, stratify=df.label.values\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df, valid_df = model_selection.train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df.label.values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:31.736955Z",
     "iopub.status.busy": "2021-01-22T14:46:31.736299Z",
     "iopub.status.idle": "2021-01-22T14:46:31.766807Z",
     "shell.execute_reply": "2021-01-22T14:46:31.766269Z"
    },
    "papermill": {
     "duration": 0.092108,
     "end_time": "2021-01-22T14:46:31.766921",
     "exception": false,
     "start_time": "2021-01-22T14:46:31.674813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"class CassavaDataset(torch.utils.data.Dataset):\\n    \\\"\\\"\\\"\\n    Dataset\\uc744 \\uad6c\\ucd95\\ud558\\uae30 \\uc704\\ud55c \\uae30\\ubcf8 \\ucf54\\ub4dc\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, df, data_path=DATA_PATH, mode=\\\"train\\\", transforms=None):\\n        super().__init__()\\n        self.df_data = df.values\\n        self.data_path = data_path\\n        self.transforms = transforms\\n        self.mode = mode\\n        self.data_dir = \\\"train_images\\\" if mode == \\\"train\\\" else \\\"test_images\\\"\\n\\n    def __len__(self):\\n        return len(self.df_data)\\n\\n    def __getitem__(self, index):\\n        img_name, label = self.df_data[index]\\n        img_path = os.path.join(self.data_path, self.data_dir, img_name)\\n        img = Image.open(img_path).convert(\\\"RGB\\\")\\n\\n        if self.transforms is not None:\\n            image = self.transforms(img)\\n\\n        return image, label\";\n",
       "                var nbb_formatted_code = \"class CassavaDataset(torch.utils.data.Dataset):\\n    \\\"\\\"\\\"\\n    Dataset\\uc744 \\uad6c\\ucd95\\ud558\\uae30 \\uc704\\ud55c \\uae30\\ubcf8 \\ucf54\\ub4dc\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, df, data_path=DATA_PATH, mode=\\\"train\\\", transforms=None):\\n        super().__init__()\\n        self.df_data = df.values\\n        self.data_path = data_path\\n        self.transforms = transforms\\n        self.mode = mode\\n        self.data_dir = \\\"train_images\\\" if mode == \\\"train\\\" else \\\"test_images\\\"\\n\\n    def __len__(self):\\n        return len(self.df_data)\\n\\n    def __getitem__(self, index):\\n        img_name, label = self.df_data[index]\\n        img_path = os.path.join(self.data_path, self.data_dir, img_name)\\n        img = Image.open(img_path).convert(\\\"RGB\\\")\\n\\n        if self.transforms is not None:\\n            image = self.transforms(img)\\n\\n        return image, label\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class CassavaDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset을 구축하기 위한 기본 코드\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, data_path=DATA_PATH, mode=\"train\", transforms=None):\n",
    "        super().__init__()\n",
    "        self.df_data = df.values\n",
    "        self.data_path = data_path\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "        self.data_dir = \"train_images\" if mode == \"train\" else \"test_images\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name, label = self.df_data[index]\n",
    "        img_path = os.path.join(self.data_path, self.data_dir, img_name)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(img)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:31.887080Z",
     "iopub.status.busy": "2021-01-22T14:46:31.885997Z",
     "iopub.status.idle": "2021-01-22T14:46:31.911660Z",
     "shell.execute_reply": "2021-01-22T14:46:31.912209Z"
    },
    "papermill": {
     "duration": 0.087015,
     "end_time": "2021-01-22T14:46:31.912367",
     "exception": false,
     "start_time": "2021-01-22T14:46:31.825352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# image augmentation \\uc0dd\\uc131\\ntransforms_train = transforms.Compose(\\n    [\\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\\n        transforms.RandomHorizontalFlip(p=0.3),\\n        transforms.RandomVerticalFlip(p=0.3),\\n        transforms.RandomResizedCrop(IMG_SIZE),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\\n    ]\\n)\\n\\ntransforms_valid = transforms.Compose(\\n    [\\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\\n    ]\\n)\";\n",
       "                var nbb_formatted_code = \"# image augmentation \\uc0dd\\uc131\\ntransforms_train = transforms.Compose(\\n    [\\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\\n        transforms.RandomHorizontalFlip(p=0.3),\\n        transforms.RandomVerticalFlip(p=0.3),\\n        transforms.RandomResizedCrop(IMG_SIZE),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\\n    ]\\n)\\n\\ntransforms_valid = transforms.Compose(\\n    [\\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\\n        transforms.ToTensor(),\\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\\n    ]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# image augmentation 생성\n",
    "transforms_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomVerticalFlip(p=0.3),\n",
    "        transforms.RandomResizedCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transforms_valid = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:32.034763Z",
     "iopub.status.busy": "2021-01-22T14:46:32.034063Z",
     "iopub.status.idle": "2021-01-22T14:46:32.047055Z",
     "shell.execute_reply": "2021-01-22T14:46:32.046387Z"
    },
    "papermill": {
     "duration": 0.074654,
     "end_time": "2021-01-22T14:46:32.047214",
     "exception": false,
     "start_time": "2021-01-22T14:46:31.972560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Vision Transformer Models: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['vit_base_patch16_224',\n",
       " 'vit_base_patch16_384',\n",
       " 'vit_base_patch32_384',\n",
       " 'vit_base_resnet26d_224',\n",
       " 'vit_base_resnet50d_224',\n",
       " 'vit_huge_patch16_224',\n",
       " 'vit_huge_patch32_384',\n",
       " 'vit_large_patch16_224',\n",
       " 'vit_large_patch16_384',\n",
       " 'vit_large_patch32_384',\n",
       " 'vit_small_patch16_224',\n",
       " 'vit_small_resnet26d_224',\n",
       " 'vit_small_resnet50d_s3_224']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"print(\\\"Available Vision Transformer Models: \\\")\\ntimm.list_models(\\\"vit*\\\")\";\n",
       "                var nbb_formatted_code = \"print(\\\"Available Vision Transformer Models: \\\")\\ntimm.list_models(\\\"vit*\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Available Vision Transformer Models: \")\n",
    "timm.list_models(\"vit*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:32.174424Z",
     "iopub.status.busy": "2021-01-22T14:46:32.173603Z",
     "iopub.status.idle": "2021-01-22T14:46:32.250410Z",
     "shell.execute_reply": "2021-01-22T14:46:32.249717Z"
    },
    "papermill": {
     "duration": 0.141787,
     "end_time": "2021-01-22T14:46:32.250547",
     "exception": false,
     "start_time": "2021-01-22T14:46:32.108760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"class ViTBase16(nn.Module):\\n    def __init__(self, n_classes, pretrained=False):\\n\\n        super(ViTBase16, self).__init__()\\n\\n        self.model = timm.create_model(\\\"vit_base_patch16_224\\\", pretrained=False)\\n        if pretrained:\\n            self.model.load_state_dict(torch.load(MODEL_PATH))\\n\\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\\n\\n    def forward(self, x):\\n        x = self.model(x)\\n        return x\\n\\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\\n        # training loss\\ub97c \\ucd94\\uc801\\ud558\\uae30 \\uc704\\ud55c \\ubcc0\\uc218\\n        epoch_loss = 0.0\\n        epoch_accuracy = 0.0\\n\\n        ###################\\n        #     \\ubaa8\\ub378 \\ud559\\uc2b5    #\\n        ###################\\n        self.model.train()\\n        for i, (data, target) in enumerate(train_loader):\\n            # CUDA\\uac00 \\uc788\\ub2e4\\uba74 gpu\\ub85c \\ub370\\uc774\\ud130\\ub97c \\uc774\\ub3d9\\n            if device.type == \\\"cuda\\\":\\n                data, target = data.cuda(), target.cuda()\\n            elif device.type == \\\"xla\\\":\\n                data = data.to(device, dtype=torch.float32)\\n                target = target.to(device, dtype=torch.int64)\\n\\n            # \\ubaa8\\ub4e0 optimized variable\\uc5d0 \\ub300\\ud574 gradient \\ucd08\\uae30\\ud654\\n            optimizer.zero_grad()\\n            # forward pass: \\ubaa8\\ub378\\uc5d0 input\\uc744 \\uc785\\ub825\\ud568\\uc73c\\ub85c\\uc368 \\uc608\\uce21\\uac12 \\uacc4\\uc0b0\\n            output = self.forward(data)\\n            # batch loss \\uacc4\\uc0b0\\n            loss = criterion(output, target)\\n            # backward pass: \\ubaa8\\ub378 \\ud30c\\ub77c\\ubbf8\\ud130\\uc5d0 \\ub300\\ud55c loss\\uc758 gradient \\uacc4\\uc0b0\\n            loss.backward()\\n            # \\uc815\\ud655\\ub3c4 \\uacc4\\uc0b0\\n            accuracy = (output.argmax(dim=1) == target).float().mean()\\n            # training loss \\ubc0f \\uc815\\ud655\\ub3c4 \\uc5c5\\ub370\\uc774\\ud2b8\\n            epoch_loss += loss\\n            epoch_accuracy += accuracy\\n\\n            # \\ucd5c\\uc801\\ud654 \\ub2e8\\uacc4 (\\ud30c\\ub77c\\ubbf8\\ud130 \\uc5c5\\ub370\\uc774\\ud2b8)\\n            if device.type == \\\"xla\\\":\\n                xm.optimizer_step(optimizer)\\n\\n                if i % 20 == 0:\\n                    xm.master_print(f\\\"\\\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\\\")\\n\\n            else:\\n                optimizer.step()\\n\\n        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\\n\\n    def validate_one_epoch(self, valid_loader, criterion, device):\\n        # validation loss\\ub97c \\ucd94\\uc801\\ud558\\uae30 \\uc704\\ud55c \\ubcc0\\uc218\\n        valid_loss = 0.0\\n        valid_accuracy = 0.0\\n\\n        ######################\\n        #      \\ubaa8\\ub378 \\ud3c9\\uac00      #\\n        ######################\\n        self.model.eval()\\n        for data, target in valid_loader:\\n            # CUDA\\uac00 \\uc788\\ub2e4\\uba74 gpu\\ub85c \\ub370\\uc774\\ud130\\ub97c \\uc774\\ub3d9\\n            if device.type == \\\"cuda\\\":\\n                data, target = data.cuda(), target.cuda()\\n            elif device.type == \\\"xla\\\":\\n                data = data.to(device, dtype=torch.float32)\\n                target = target.to(device, dtype=torch.int64)\\n\\n            with torch.no_grad():\\n                # forward pass: \\ubaa8\\ub378\\uc5d0 input\\uc744 \\uc785\\ub825\\ud568\\uc73c\\ub85c\\uc368 \\uc608\\uce21\\uac12 \\uacc4\\uc0b0\\n                output = self.model(data)\\n                # batch loss \\uacc4\\uc0b0\\n                loss = criterion(output, target)\\n                # \\uc815\\ud655\\ub3c4 \\uacc4\\uc0b0\\n                accuracy = (output.argmax(dim=1) == target).float().mean()\\n                # validation loss\\uc640 \\uc815\\ud655\\ub3c4 \\uc5c5\\ub370\\uc774\\ud2b8\\n                valid_loss += loss\\n                valid_accuracy += accuracy\\n\\n        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)\";\n",
       "                var nbb_formatted_code = \"class ViTBase16(nn.Module):\\n    def __init__(self, n_classes, pretrained=False):\\n\\n        super(ViTBase16, self).__init__()\\n\\n        self.model = timm.create_model(\\\"vit_base_patch16_224\\\", pretrained=False)\\n        if pretrained:\\n            self.model.load_state_dict(torch.load(MODEL_PATH))\\n\\n        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\\n\\n    def forward(self, x):\\n        x = self.model(x)\\n        return x\\n\\n    def train_one_epoch(self, train_loader, criterion, optimizer, device):\\n        # training loss\\ub97c \\ucd94\\uc801\\ud558\\uae30 \\uc704\\ud55c \\ubcc0\\uc218\\n        epoch_loss = 0.0\\n        epoch_accuracy = 0.0\\n\\n        ###################\\n        #     \\ubaa8\\ub378 \\ud559\\uc2b5    #\\n        ###################\\n        self.model.train()\\n        for i, (data, target) in enumerate(train_loader):\\n            # CUDA\\uac00 \\uc788\\ub2e4\\uba74 gpu\\ub85c \\ub370\\uc774\\ud130\\ub97c \\uc774\\ub3d9\\n            if device.type == \\\"cuda\\\":\\n                data, target = data.cuda(), target.cuda()\\n            elif device.type == \\\"xla\\\":\\n                data = data.to(device, dtype=torch.float32)\\n                target = target.to(device, dtype=torch.int64)\\n\\n            # \\ubaa8\\ub4e0 optimized variable\\uc5d0 \\ub300\\ud574 gradient \\ucd08\\uae30\\ud654\\n            optimizer.zero_grad()\\n            # forward pass: \\ubaa8\\ub378\\uc5d0 input\\uc744 \\uc785\\ub825\\ud568\\uc73c\\ub85c\\uc368 \\uc608\\uce21\\uac12 \\uacc4\\uc0b0\\n            output = self.forward(data)\\n            # batch loss \\uacc4\\uc0b0\\n            loss = criterion(output, target)\\n            # backward pass: \\ubaa8\\ub378 \\ud30c\\ub77c\\ubbf8\\ud130\\uc5d0 \\ub300\\ud55c loss\\uc758 gradient \\uacc4\\uc0b0\\n            loss.backward()\\n            # \\uc815\\ud655\\ub3c4 \\uacc4\\uc0b0\\n            accuracy = (output.argmax(dim=1) == target).float().mean()\\n            # training loss \\ubc0f \\uc815\\ud655\\ub3c4 \\uc5c5\\ub370\\uc774\\ud2b8\\n            epoch_loss += loss\\n            epoch_accuracy += accuracy\\n\\n            # \\ucd5c\\uc801\\ud654 \\ub2e8\\uacc4 (\\ud30c\\ub77c\\ubbf8\\ud130 \\uc5c5\\ub370\\uc774\\ud2b8)\\n            if device.type == \\\"xla\\\":\\n                xm.optimizer_step(optimizer)\\n\\n                if i % 20 == 0:\\n                    xm.master_print(f\\\"\\\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\\\")\\n\\n            else:\\n                optimizer.step()\\n\\n        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\\n\\n    def validate_one_epoch(self, valid_loader, criterion, device):\\n        # validation loss\\ub97c \\ucd94\\uc801\\ud558\\uae30 \\uc704\\ud55c \\ubcc0\\uc218\\n        valid_loss = 0.0\\n        valid_accuracy = 0.0\\n\\n        ######################\\n        #      \\ubaa8\\ub378 \\ud3c9\\uac00      #\\n        ######################\\n        self.model.eval()\\n        for data, target in valid_loader:\\n            # CUDA\\uac00 \\uc788\\ub2e4\\uba74 gpu\\ub85c \\ub370\\uc774\\ud130\\ub97c \\uc774\\ub3d9\\n            if device.type == \\\"cuda\\\":\\n                data, target = data.cuda(), target.cuda()\\n            elif device.type == \\\"xla\\\":\\n                data = data.to(device, dtype=torch.float32)\\n                target = target.to(device, dtype=torch.int64)\\n\\n            with torch.no_grad():\\n                # forward pass: \\ubaa8\\ub378\\uc5d0 input\\uc744 \\uc785\\ub825\\ud568\\uc73c\\ub85c\\uc368 \\uc608\\uce21\\uac12 \\uacc4\\uc0b0\\n                output = self.model(data)\\n                # batch loss \\uacc4\\uc0b0\\n                loss = criterion(output, target)\\n                # \\uc815\\ud655\\ub3c4 \\uacc4\\uc0b0\\n                accuracy = (output.argmax(dim=1) == target).float().mean()\\n                # validation loss\\uc640 \\uc815\\ud655\\ub3c4 \\uc5c5\\ub370\\uc774\\ud2b8\\n                valid_loss += loss\\n                valid_accuracy += accuracy\\n\\n        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ViTBase16(nn.Module):\n",
    "    def __init__(self, n_classes, pretrained=False):\n",
    "\n",
    "        super(ViTBase16, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=False)\n",
    "        if pretrained:\n",
    "            self.model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n",
    "        # training loss를 추적하기 위한 변수\n",
    "        epoch_loss = 0.0\n",
    "        epoch_accuracy = 0.0\n",
    "\n",
    "        ###################\n",
    "        #     모델 학습    #\n",
    "        ###################\n",
    "        self.model.train()\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "            # CUDA가 있다면 gpu로 데이터를 이동\n",
    "            if device.type == \"cuda\":\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            elif device.type == \"xla\":\n",
    "                data = data.to(device, dtype=torch.float32)\n",
    "                target = target.to(device, dtype=torch.int64)\n",
    "\n",
    "            # 모든 optimized variable에 대해 gradient 초기화\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: 모델에 input을 입력함으로써 예측값 계산\n",
    "            output = self.forward(data)\n",
    "            # batch loss 계산\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: 모델 파라미터에 대한 loss의 gradient 계산\n",
    "            loss.backward()\n",
    "            # 정확도 계산\n",
    "            accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "            # training loss 및 정확도 업데이트\n",
    "            epoch_loss += loss\n",
    "            epoch_accuracy += accuracy\n",
    "\n",
    "            # 최적화 단계 (파라미터 업데이트)\n",
    "            if device.type == \"xla\":\n",
    "                xm.optimizer_step(optimizer)\n",
    "\n",
    "                if i % 20 == 0:\n",
    "                    xm.master_print(f\"\\tBATCH {i+1}/{len(train_loader)} - LOSS: {loss}\")\n",
    "\n",
    "            else:\n",
    "                optimizer.step()\n",
    "\n",
    "        return epoch_loss / len(train_loader), epoch_accuracy / len(train_loader)\n",
    "\n",
    "    def validate_one_epoch(self, valid_loader, criterion, device):\n",
    "        # validation loss를 추적하기 위한 변수\n",
    "        valid_loss = 0.0\n",
    "        valid_accuracy = 0.0\n",
    "\n",
    "        ######################\n",
    "        #      모델 평가      #\n",
    "        ######################\n",
    "        self.model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            # CUDA가 있다면 gpu로 데이터를 이동\n",
    "            if device.type == \"cuda\":\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            elif device.type == \"xla\":\n",
    "                data = data.to(device, dtype=torch.float32)\n",
    "                target = target.to(device, dtype=torch.int64)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # forward pass: 모델에 input을 입력함으로써 예측값 계산\n",
    "                output = self.model(data)\n",
    "                # batch loss 계산\n",
    "                loss = criterion(output, target)\n",
    "                # 정확도 계산\n",
    "                accuracy = (output.argmax(dim=1) == target).float().mean()\n",
    "                # validation loss와 정확도 업데이트\n",
    "                valid_loss += loss\n",
    "                valid_accuracy += accuracy\n",
    "\n",
    "        return valid_loss / len(valid_loader), valid_accuracy / len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:32.388643Z",
     "iopub.status.busy": "2021-01-22T14:46:32.387924Z",
     "iopub.status.idle": "2021-01-22T14:46:32.422761Z",
     "shell.execute_reply": "2021-01-22T14:46:32.422204Z"
    },
    "papermill": {
     "duration": 0.112207,
     "end_time": "2021-01-22T14:46:32.422875",
     "exception": false,
     "start_time": "2021-01-22T14:46:32.310668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"def fit_tpu(\\n    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\\n):\\n\\n    valid_loss_min = np.Inf  # validation loss\\uc758 \\ubcc0\\ud654 \\ucd94\\uc801\\uc744 \\uc704\\ud55c \\ubcc0\\uc218\\n\\n    # loss \\ucd94\\uc801\\uc744 \\uc704\\ud55c \\ubcc0\\uc218\\ub4e4\\n    train_losses = []\\n    valid_losses = []\\n    train_accs = []\\n    valid_accs = []\\n\\n    for epoch in range(1, epochs + 1):\\n        gc.collect()\\n        para_train_loader = pl.ParallelLoader(train_loader, [device])\\n\\n        xm.master_print(f\\\"{'='*50}\\\")\\n        xm.master_print(f\\\"EPOCH {epoch} - TRAINING...\\\")\\n        train_loss, train_acc = model.train_one_epoch(\\n            para_train_loader.per_device_loader(device), criterion, optimizer, device\\n        )\\n        xm.master_print(\\n            f\\\"\\\\n\\\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\\\n\\\"\\n        )\\n        train_losses.append(train_loss)\\n        train_accs.append(train_acc)\\n        gc.collect()\\n\\n        if valid_loader is not None:\\n            gc.collect()\\n            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\\n            xm.master_print(f\\\"EPOCH {epoch} - VALIDATING...\\\")\\n            valid_loss, valid_acc = model.validate_one_epoch(\\n                para_valid_loader.per_device_loader(device), criterion, device\\n            )\\n            xm.master_print(f\\\"\\\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\\\n\\\")\\n            valid_losses.append(valid_loss)\\n            valid_accs.append(valid_acc)\\n            gc.collect()\\n\\n            # validation loss\\uac00 \\uc904\\uc5b4\\ub4e4\\uba74 \\ubaa8\\ub378 \\uc800\\uc7a5\\n            if valid_loss <= valid_loss_min and epoch != 1:\\n                xm.master_print(\\n                    \\\"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\\\".format(\\n                        valid_loss_min, valid_loss\\n                    )\\n                )\\n            #                 xm.save(model.state_dict(), 'best_model.pth')\\n\\n            valid_loss_min = valid_loss\\n\\n    return {\\n        \\\"train_loss\\\": train_losses,\\n        \\\"valid_losses\\\": valid_losses,\\n        \\\"train_acc\\\": train_accs,\\n        \\\"valid_acc\\\": valid_accs,\\n    }\";\n",
       "                var nbb_formatted_code = \"def fit_tpu(\\n    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\\n):\\n\\n    valid_loss_min = np.Inf  # validation loss\\uc758 \\ubcc0\\ud654 \\ucd94\\uc801\\uc744 \\uc704\\ud55c \\ubcc0\\uc218\\n\\n    # loss \\ucd94\\uc801\\uc744 \\uc704\\ud55c \\ubcc0\\uc218\\ub4e4\\n    train_losses = []\\n    valid_losses = []\\n    train_accs = []\\n    valid_accs = []\\n\\n    for epoch in range(1, epochs + 1):\\n        gc.collect()\\n        para_train_loader = pl.ParallelLoader(train_loader, [device])\\n\\n        xm.master_print(f\\\"{'='*50}\\\")\\n        xm.master_print(f\\\"EPOCH {epoch} - TRAINING...\\\")\\n        train_loss, train_acc = model.train_one_epoch(\\n            para_train_loader.per_device_loader(device), criterion, optimizer, device\\n        )\\n        xm.master_print(\\n            f\\\"\\\\n\\\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\\\n\\\"\\n        )\\n        train_losses.append(train_loss)\\n        train_accs.append(train_acc)\\n        gc.collect()\\n\\n        if valid_loader is not None:\\n            gc.collect()\\n            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\\n            xm.master_print(f\\\"EPOCH {epoch} - VALIDATING...\\\")\\n            valid_loss, valid_acc = model.validate_one_epoch(\\n                para_valid_loader.per_device_loader(device), criterion, device\\n            )\\n            xm.master_print(f\\\"\\\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\\\n\\\")\\n            valid_losses.append(valid_loss)\\n            valid_accs.append(valid_acc)\\n            gc.collect()\\n\\n            # validation loss\\uac00 \\uc904\\uc5b4\\ub4e4\\uba74 \\ubaa8\\ub378 \\uc800\\uc7a5\\n            if valid_loss <= valid_loss_min and epoch != 1:\\n                xm.master_print(\\n                    \\\"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\\\".format(\\n                        valid_loss_min, valid_loss\\n                    )\\n                )\\n            #                 xm.save(model.state_dict(), 'best_model.pth')\\n\\n            valid_loss_min = valid_loss\\n\\n    return {\\n        \\\"train_loss\\\": train_losses,\\n        \\\"valid_losses\\\": valid_losses,\\n        \\\"train_acc\\\": train_accs,\\n        \\\"valid_acc\\\": valid_accs,\\n    }\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_tpu(\n",
    "    model, epochs, device, criterion, optimizer, train_loader, valid_loader=None\n",
    "):\n",
    "\n",
    "    valid_loss_min = np.Inf  # validation loss의 변화 추적을 위한 변수\n",
    "\n",
    "    # loss 추적을 위한 변수들\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    train_accs = []\n",
    "    valid_accs = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        gc.collect()\n",
    "        para_train_loader = pl.ParallelLoader(train_loader, [device])\n",
    "\n",
    "        xm.master_print(f\"{'='*50}\")\n",
    "        xm.master_print(f\"EPOCH {epoch} - TRAINING...\")\n",
    "        train_loss, train_acc = model.train_one_epoch(\n",
    "            para_train_loader.per_device_loader(device), criterion, optimizer, device\n",
    "        )\n",
    "        xm.master_print(\n",
    "            f\"\\n\\t[TRAIN] EPOCH {epoch} - LOSS: {train_loss}, ACCURACY: {train_acc}\\n\"\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        gc.collect()\n",
    "\n",
    "        if valid_loader is not None:\n",
    "            gc.collect()\n",
    "            para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n",
    "            xm.master_print(f\"EPOCH {epoch} - VALIDATING...\")\n",
    "            valid_loss, valid_acc = model.validate_one_epoch(\n",
    "                para_valid_loader.per_device_loader(device), criterion, device\n",
    "            )\n",
    "            xm.master_print(f\"\\t[VALID] LOSS: {valid_loss}, ACCURACY: {valid_acc}\\n\")\n",
    "            valid_losses.append(valid_loss)\n",
    "            valid_accs.append(valid_acc)\n",
    "            gc.collect()\n",
    "\n",
    "            # validation loss가 줄어들면 모델 저장\n",
    "            if valid_loss <= valid_loss_min and epoch != 1:\n",
    "                xm.master_print(\n",
    "                    \"Validation loss decreased ({:.4f} --> {:.4f}).  Saving model ...\".format(\n",
    "                        valid_loss_min, valid_loss\n",
    "                    )\n",
    "                )\n",
    "            #                 xm.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_losses,\n",
    "        \"valid_losses\": valid_losses,\n",
    "        \"train_acc\": train_accs,\n",
    "        \"valid_acc\": valid_accs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:32.548924Z",
     "iopub.status.busy": "2021-01-22T14:46:32.548286Z",
     "iopub.status.idle": "2021-01-22T14:46:39.486893Z",
     "shell.execute_reply": "2021-01-22T14:46:39.486355Z"
    },
    "papermill": {
     "duration": 7.00394,
     "end_time": "2021-01-22T14:46:39.487027",
     "exception": false,
     "start_time": "2021-01-22T14:46:32.483087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"model = ViTBase16(n_classes=5, pretrained=True)\";\n",
       "                var nbb_formatted_code = \"model = ViTBase16(n_classes=5, pretrained=True)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ViTBase16(n_classes=5, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:39.632640Z",
     "iopub.status.busy": "2021-01-22T14:46:39.631579Z",
     "iopub.status.idle": "2021-01-22T14:46:39.666202Z",
     "shell.execute_reply": "2021-01-22T14:46:39.666749Z"
    },
    "papermill": {
     "duration": 0.115065,
     "end_time": "2021-01-22T14:46:39.666893",
     "exception": false,
     "start_time": "2021-01-22T14:46:39.551828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"def _run():\\n    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\\n    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\\n\\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\\n        train_dataset,\\n        num_replicas=xm.xrt_world_size(),\\n        rank=xm.get_ordinal(),\\n        shuffle=True,\\n    )\\n\\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\\n        valid_dataset,\\n        num_replicas=xm.xrt_world_size(),\\n        rank=xm.get_ordinal(),\\n        shuffle=False,\\n    )\\n\\n    train_loader = torch.utils.data.DataLoader(\\n        dataset=train_dataset,\\n        batch_size=BATCH_SIZE,\\n        sampler=train_sampler,\\n        drop_last=True,\\n        num_workers=8,\\n    )\\n\\n    valid_loader = torch.utils.data.DataLoader(\\n        dataset=valid_dataset,\\n        batch_size=BATCH_SIZE,\\n        sampler=valid_sampler,\\n        drop_last=True,\\n        num_workers=8,\\n    )\\n\\n    criterion = nn.CrossEntropyLoss()\\n    #     device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n    device = xm.xla_device()\\n    model.to(device)\\n\\n    lr = LR * xm.xrt_world_size()\\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n\\n    xm.master_print(f\\\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\\\")\\n    start_time = datetime.now()\\n    xm.master_print(f\\\"Start Time: {start_time}\\\")\\n\\n    logs = fit_tpu(\\n        model=model,\\n        epochs=N_EPOCHS,\\n        device=device,\\n        criterion=criterion,\\n        optimizer=optimizer,\\n        train_loader=train_loader,\\n        valid_loader=valid_loader,\\n    )\\n\\n    xm.master_print(f\\\"Execution time: {datetime.now() - start_time}\\\")\\n\\n    xm.master_print(\\\"Saving Model\\\")\\n    xm.save(\\n        model.state_dict(), f'model_5e_{datetime.now().strftime(\\\"%Y%m%d-%H%M\\\")}.pth'\\n    )\";\n",
       "                var nbb_formatted_code = \"def _run():\\n    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\\n    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\\n\\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\\n        train_dataset,\\n        num_replicas=xm.xrt_world_size(),\\n        rank=xm.get_ordinal(),\\n        shuffle=True,\\n    )\\n\\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\\n        valid_dataset,\\n        num_replicas=xm.xrt_world_size(),\\n        rank=xm.get_ordinal(),\\n        shuffle=False,\\n    )\\n\\n    train_loader = torch.utils.data.DataLoader(\\n        dataset=train_dataset,\\n        batch_size=BATCH_SIZE,\\n        sampler=train_sampler,\\n        drop_last=True,\\n        num_workers=8,\\n    )\\n\\n    valid_loader = torch.utils.data.DataLoader(\\n        dataset=valid_dataset,\\n        batch_size=BATCH_SIZE,\\n        sampler=valid_sampler,\\n        drop_last=True,\\n        num_workers=8,\\n    )\\n\\n    criterion = nn.CrossEntropyLoss()\\n    #     device = torch.device(\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\")\\n    device = xm.xla_device()\\n    model.to(device)\\n\\n    lr = LR * xm.xrt_world_size()\\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n\\n    xm.master_print(f\\\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\\\")\\n    start_time = datetime.now()\\n    xm.master_print(f\\\"Start Time: {start_time}\\\")\\n\\n    logs = fit_tpu(\\n        model=model,\\n        epochs=N_EPOCHS,\\n        device=device,\\n        criterion=criterion,\\n        optimizer=optimizer,\\n        train_loader=train_loader,\\n        valid_loader=valid_loader,\\n    )\\n\\n    xm.master_print(f\\\"Execution time: {datetime.now() - start_time}\\\")\\n\\n    xm.master_print(\\\"Saving Model\\\")\\n    xm.save(\\n        model.state_dict(), f'model_5e_{datetime.now().strftime(\\\"%Y%m%d-%H%M\\\")}.pth'\\n    )\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _run():\n",
    "    train_dataset = CassavaDataset(train_df, transforms=transforms_train)\n",
    "    valid_dataset = CassavaDataset(valid_df, transforms=transforms_valid)\n",
    "\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        valid_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset=valid_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=valid_sampler,\n",
    "        drop_last=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = xm.xla_device()\n",
    "    model.to(device)\n",
    "\n",
    "    lr = LR * xm.xrt_world_size()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    xm.master_print(f\"INITIALIZING TRAINING ON {xm.xrt_world_size()} TPU CORES\")\n",
    "    start_time = datetime.now()\n",
    "    xm.master_print(f\"Start Time: {start_time}\")\n",
    "\n",
    "    logs = fit_tpu(\n",
    "        model=model,\n",
    "        epochs=N_EPOCHS,\n",
    "        device=device,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        valid_loader=valid_loader,\n",
    "    )\n",
    "\n",
    "    xm.master_print(f\"Execution time: {datetime.now() - start_time}\")\n",
    "\n",
    "    xm.master_print(\"Saving Model\")\n",
    "    xm.save(\n",
    "        model.state_dict(), f'model_5e_{datetime.now().strftime(\"%Y%m%d-%H%M\")}.pth'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-22T14:46:39.805030Z",
     "iopub.status.busy": "2021-01-22T14:46:39.804052Z",
     "iopub.status.idle": "2021-01-22T15:19:31.159368Z",
     "shell.execute_reply": "2021-01-22T15:19:31.159854Z"
    },
    "papermill": {
     "duration": 1971.432011,
     "end_time": "2021-01-22T15:19:31.160110",
     "exception": false,
     "start_time": "2021-01-22T14:46:39.728099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZING TRAINING ON 8 TPU CORES\n",
      "Start Time: 2021-01-22 14:46:48.232790\n",
      "==================================================\n",
      "EPOCH 1 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 1.609375\n",
      "\tBATCH 21/150 - LOSS: 0.7734375\n",
      "\tBATCH 41/150 - LOSS: 0.33984375\n",
      "\tBATCH 61/150 - LOSS: 0.333984375\n",
      "\tBATCH 81/150 - LOSS: 0.314453125\n",
      "\tBATCH 101/150 - LOSS: 0.765625\n",
      "\tBATCH 121/150 - LOSS: 0.439453125\n",
      "\tBATCH 141/150 - LOSS: 0.451171875\n",
      "\n",
      "\t[TRAIN] EPOCH 1 - LOSS: 0.59765625, ACCURACY: 0.83203125\n",
      "\n",
      "EPOCH 1 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.419921875, ACCURACY: 0.84375\n",
      "\n",
      "==================================================\n",
      "EPOCH 2 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.458984375\n",
      "\tBATCH 21/150 - LOSS: 0.2734375\n",
      "\tBATCH 41/150 - LOSS: 0.1396484375\n",
      "\tBATCH 61/150 - LOSS: 0.177734375\n",
      "\tBATCH 81/150 - LOSS: 0.14453125\n",
      "\tBATCH 101/150 - LOSS: 0.5703125\n",
      "\tBATCH 121/150 - LOSS: 0.32421875\n",
      "\tBATCH 141/150 - LOSS: 0.60546875\n",
      "\n",
      "\t[TRAIN] EPOCH 2 - LOSS: 0.41796875, ACCURACY: 0.9140625\n",
      "\n",
      "EPOCH 2 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.4140625, ACCURACY: 0.83984375\n",
      "\n",
      "Validation loss decreased (0.4199 --> 0.4141).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 3 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.20703125\n",
      "\tBATCH 21/150 - LOSS: 0.5625\n",
      "\tBATCH 41/150 - LOSS: 0.15625\n",
      "\tBATCH 61/150 - LOSS: 0.2373046875\n",
      "\tBATCH 81/150 - LOSS: 0.208984375\n",
      "\tBATCH 101/150 - LOSS: 0.435546875\n",
      "\tBATCH 121/150 - LOSS: 0.28515625\n",
      "\tBATCH 141/150 - LOSS: 0.337890625\n",
      "\n",
      "\t[TRAIN] EPOCH 3 - LOSS: 0.39453125, ACCURACY: 0.9140625\n",
      "\n",
      "EPOCH 3 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.404296875, ACCURACY: 0.83984375\n",
      "\n",
      "Validation loss decreased (0.4141 --> 0.4043).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 4 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.17578125\n",
      "\tBATCH 21/150 - LOSS: 0.15625\n",
      "\tBATCH 41/150 - LOSS: 0.154296875\n",
      "\tBATCH 61/150 - LOSS: 0.11865234375\n",
      "\tBATCH 81/150 - LOSS: 0.1298828125\n",
      "\tBATCH 101/150 - LOSS: 0.36328125\n",
      "\tBATCH 121/150 - LOSS: 0.208984375\n",
      "\tBATCH 141/150 - LOSS: 0.333984375\n",
      "\n",
      "\t[TRAIN] EPOCH 4 - LOSS: 0.369140625, ACCURACY: 0.921875\n",
      "\n",
      "EPOCH 4 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.3984375, ACCURACY: 0.859375\n",
      "\n",
      "Validation loss decreased (0.4043 --> 0.3984).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 5 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.5390625\n",
      "\tBATCH 21/150 - LOSS: 0.1865234375\n",
      "\tBATCH 41/150 - LOSS: 0.263671875\n",
      "\tBATCH 61/150 - LOSS: 0.1328125\n",
      "\tBATCH 81/150 - LOSS: 0.10498046875\n",
      "\tBATCH 101/150 - LOSS: 0.390625\n",
      "\tBATCH 121/150 - LOSS: 0.2890625\n",
      "\tBATCH 141/150 - LOSS: 0.318359375\n",
      "\n",
      "\t[TRAIN] EPOCH 5 - LOSS: 0.357421875, ACCURACY: 0.921875\n",
      "\n",
      "EPOCH 5 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.416015625, ACCURACY: 0.84375\n",
      "\n",
      "==================================================\n",
      "EPOCH 6 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.51953125\n",
      "\tBATCH 21/150 - LOSS: 0.27734375\n",
      "\tBATCH 41/150 - LOSS: 0.2138671875\n",
      "\tBATCH 61/150 - LOSS: 0.1630859375\n",
      "\tBATCH 81/150 - LOSS: 0.19140625\n",
      "\tBATCH 101/150 - LOSS: 0.60546875\n",
      "\tBATCH 121/150 - LOSS: 0.271484375\n",
      "\tBATCH 141/150 - LOSS: 0.1943359375\n",
      "\n",
      "\t[TRAIN] EPOCH 6 - LOSS: 0.34375, ACCURACY: 0.92578125\n",
      "\n",
      "EPOCH 6 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.41796875, ACCURACY: 0.8515625\n",
      "\n",
      "==================================================\n",
      "EPOCH 7 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.3125\n",
      "\tBATCH 21/150 - LOSS: 0.30078125\n",
      "\tBATCH 41/150 - LOSS: 0.2041015625\n",
      "\tBATCH 61/150 - LOSS: 0.3515625\n",
      "\tBATCH 81/150 - LOSS: 0.0849609375\n",
      "\tBATCH 101/150 - LOSS: 0.408203125\n",
      "\tBATCH 121/150 - LOSS: 0.146484375\n",
      "\tBATCH 141/150 - LOSS: 0.2001953125\n",
      "\n",
      "\t[TRAIN] EPOCH 7 - LOSS: 0.30078125, ACCURACY: 0.94140625\n",
      "\n",
      "EPOCH 7 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.40625, ACCURACY: 0.85546875\n",
      "\n",
      "Validation loss decreased (0.4180 --> 0.4062).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 8 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.16015625\n",
      "\tBATCH 21/150 - LOSS: 0.181640625\n",
      "\tBATCH 41/150 - LOSS: 0.083984375\n",
      "\tBATCH 61/150 - LOSS: 0.142578125\n",
      "\tBATCH 81/150 - LOSS: 0.11962890625\n",
      "\tBATCH 101/150 - LOSS: 0.2197265625\n",
      "\tBATCH 121/150 - LOSS: 0.1875\n",
      "\tBATCH 141/150 - LOSS: 0.33984375\n",
      "\n",
      "\t[TRAIN] EPOCH 8 - LOSS: 0.28515625, ACCURACY: 0.94140625\n",
      "\n",
      "EPOCH 8 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.40625, ACCURACY: 0.87109375\n",
      "\n",
      "Validation loss decreased (0.4062 --> 0.4062).  Saving model ...\n",
      "==================================================\n",
      "EPOCH 9 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.435546875\n",
      "\tBATCH 21/150 - LOSS: 0.39453125\n",
      "\tBATCH 41/150 - LOSS: 0.212890625\n",
      "\tBATCH 61/150 - LOSS: 0.0830078125\n",
      "\tBATCH 81/150 - LOSS: 0.07958984375\n",
      "\tBATCH 101/150 - LOSS: 0.34765625\n",
      "\tBATCH 121/150 - LOSS: 0.279296875\n",
      "\tBATCH 141/150 - LOSS: 0.2470703125\n",
      "\n",
      "\t[TRAIN] EPOCH 9 - LOSS: 0.2890625, ACCURACY: 0.9453125\n",
      "\n",
      "EPOCH 9 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.41796875, ACCURACY: 0.87109375\n",
      "\n",
      "==================================================\n",
      "EPOCH 10 - TRAINING...\n",
      "\tBATCH 1/150 - LOSS: 0.38671875\n",
      "\tBATCH 21/150 - LOSS: 0.2119140625\n",
      "\tBATCH 41/150 - LOSS: 0.06884765625\n",
      "\tBATCH 61/150 - LOSS: 0.1806640625\n",
      "\tBATCH 81/150 - LOSS: 0.0546875\n",
      "\tBATCH 101/150 - LOSS: 0.19140625\n",
      "\tBATCH 121/150 - LOSS: 0.197265625\n",
      "\tBATCH 141/150 - LOSS: 0.328125\n",
      "\n",
      "\t[TRAIN] EPOCH 10 - LOSS: 0.28125, ACCURACY: 0.9453125\n",
      "\n",
      "EPOCH 10 - VALIDATING...\n",
      "\t[VALID] LOSS: 0.453125, ACCURACY: 0.83984375\n",
      "\n",
      "Execution time: 0:32:36.244843\n",
      "Saving Model\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# \\ud559\\uc2b5 \\uc2dc\\uc791\\ndef _mp_fn(rank, flags):\\n    torch.set_default_tensor_type(\\\"torch.FloatTensor\\\")\\n    a = _run()\\n\\n\\n# _run()\\nFLAGS = {}\\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\\\"fork\\\")\";\n",
       "                var nbb_formatted_code = \"# \\ud559\\uc2b5 \\uc2dc\\uc791\\ndef _mp_fn(rank, flags):\\n    torch.set_default_tensor_type(\\\"torch.FloatTensor\\\")\\n    a = _run()\\n\\n\\n# _run()\\nFLAGS = {}\\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\\\"fork\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 시작\n",
    "def _mp_fn(rank, flags):\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "    a = _run()\n",
    "\n",
    "\n",
    "# _run()\n",
    "FLAGS = {}\n",
    "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method=\"fork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.113224,
     "end_time": "2021-01-22T15:19:31.388481",
     "exception": false,
     "start_time": "2021-01-22T15:19:31.275257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 읽어주셔서 감사합니다.\n",
    "\n",
    "# <font size=4 color='blue'>이 노트북이 도움이 된다면, 추천 부탁드립니다.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.112904,
     "end_time": "2021-01-22T15:19:31.614547",
     "exception": false,
     "start_time": "2021-01-22T15:19:31.501643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 2101.085287,
   "end_time": "2021-01-22T15:19:31.840736",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-22T14:44:30.755449",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
